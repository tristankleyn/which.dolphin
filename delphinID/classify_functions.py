# -*- coding: utf-8 -*-
"""classify_functions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BLqTmRAZ14uzFRgvUwH4VM7QfJ-xUrbq
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import cm
import seaborn as sns
import shutil
import random

from colorama import Fore
import shap

from tqdm import tqdm
from PIL import Image, ImageOps
import itertools

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow import keras
from keras import Sequential,Input,Model,regularizers
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D
from keras.layers import BatchNormalization
from keras.layers import LeakyReLU
from tensorflow.keras.applications.vgg19 import VGG19
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg19 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.layers import BatchNormalization
from keras.callbacks import EarlyStopping
from tensorflow.keras import layers, models # For specifying the type of layer (Dense)
from tensorflow.keras.models import Sequential

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

folder = 'drive/MyDrive/I'

def compile_examples(folder, w=2, omit_encs = ['500'], nmin=2, THR=10, selectspecies = ['Dde', 'Ggr', 'Gme', 'Lac', 'Lal', 'Oor', 'Ttr'], create_test=True, specify_enc=None, seed=42, verbose=True):
  #summary of images in drive
  d = {'species':[], 'enc_id':[], 'arr_id':[], 'filename':[]}

  subf = f'CDsegments_{w}s_ALL'
  if verbose == True:
    print(f'Loading...')

  items = os.listdir(f'{folder}/{subf}')
  count = 0
  for item in items:
    if item[3] != "_":
      continue
    enc = item[item.index('_')+1:item.index('_')+4]
    if enc in omit_encs:
      continue
    d['species'].append(item[:3])
    d['enc_id'].append(enc)
    d['arr_id'].append(item[item.index('arr')+3:item.rindex('_')])
    d['filename'].append(item)
    count += 1

  imgs = pd.DataFrame(d)
  imgs = imgs[imgs.species.isin(selectspecies)].reset_index().drop(columns=['index'])
  imgs_ = pd.DataFrame(d)
  imgs_ = imgs_[imgs_.species.isin(selectspecies)].reset_index().drop(columns=['index'])

  items = np.array([int(f[f.rindex('_')+1:f.rindex('.')]) for f in imgs.filename])
  items_ = np.array([int(f[f.rindex('_')+1:f.rindex('.')]) for f in imgs.filename])
  keep = items > w*nmin
  keep_ = items_ > w*nmin
  imgs = imgs.iloc[keep, :].reset_index().drop(columns=['index'])
  imgs_ = imgs_.iloc[keep_, :].reset_index().drop(columns=['index'])

  if verbose == True:
    print(f'Done. Loaded {count} examples')

  #remove encounters with less than THR examples
  n0 = len(np.unique(imgs.enc_id))
  for enc in np.unique(imgs.enc_id):
    N = len(imgs[imgs.enc_id==enc])
    if N < THR:
      inds = imgs[imgs.enc_id==enc].index
      inds = np.array(list(inds))
      imgs = imgs.loc[~imgs.index.isin(inds)]

  imgs = imgs.reset_index().drop(columns=['index'])
  n1 = len(np.unique(imgs.enc_id))
  if verbose==True:
    print(f'Removed {n0-n1} encounters with less than {THR} examples')
    print('')
    print(f'New size of dataset: {len(imgs)} examples')

  ntestencs = 1
  mintest = 10
  testcount = {}

  imgs_species = imgs_[imgs_.species.isin(selectspecies)]
  enclist = np.unique(imgs_species.enc_id)
  random.seed(seed)

  if create_test == True:
    #clear test folder
    for item in os.listdir(f'{folder}/test'):
        os.remove(f'{folder}/test/{item}')

    if verbose == True:
      print(f'All items cleared from {folder}/test \n')

  #extract test encounters
  if specify_enc is not None:
    testenc = specify_enc
    ntest = len(imgs_[imgs_.enc_id==testenc])

  else:
    ntest = 0
    while ntest < mintest:
      np.random.shuffle(enclist)
      testenc = enclist[:1][0]
      ntest = len(imgs_[imgs_.enc_id==testenc])

  if create_test == True:
    testfiles = imgs_[imgs_.enc_id==testenc].filename.values
    testsp = list(imgs_[imgs_.enc_id==testenc]['species'])[0]
    testcount = 0

    for file in testfiles:
      src = f'{folder}/{subf}/{file}'
      dst = f'{folder}/test/{file}'
      shutil.copy(src, dst)
      testcount += 1

    inds = imgs[imgs.enc_id==testenc].index
    inds = np.array(list(inds))
    imgs = imgs.loc[~imgs.index.isin(inds)]

  if create_test == False:
    testenc = None
    testsp = 'No species'
    testcount = 0

  if verbose==True:
    print(f'Test encounter: {testenc} ({testsp})')
    print(f'Extracted {testcount} examples for {testsp} to test folder and removed from dataframe')
    print(f'\n{len(imgs)} examples for {len(np.unique(imgs.species))} species remain for training and validation')

  return imgs

def create_trainval_new(basefolder,
                    subf,
                    imgs,
                    selectspecies=['Dde', 'Ggr', 'Gme', 'Lac', 'Lal', 'Oor', 'Ttr'],
                    nmax=60,
                    split=0.33,
                    nvalencs=2,
                    seed=42,
                    verbose=False):



  # Limit number of examples per encounter
  orgsize = len(imgs)
  remove_inds = []
  for sp in np.unique(imgs.species):
    sub = imgs[imgs.species==sp]
    for enc in np.unique(sub.enc_id):
      subsub = sub[sub.enc_id==enc]
      if len(subsub) > nmax:
        diff = len(subsub) - nmax
        inds = subsub.sample(n=diff, random_state=seed).index
        inds = np.array(list(inds))
        remove_inds.extend(inds)

  imgs = imgs.loc[~imgs.index.isin(remove_inds)]
  if verbose == True:
    newsize = len(imgs)
    print(f'{orgsize-newsize} examples removed after applying encounter limit \n')

  # Clear train, val sets
  for x in ['train', 'val']:
    for subx in os.listdir(f'{folder}/{x}'):
      items = os.listdir(f'{folder}/{x}/{subx}')
      for item in items:
        os.remove(f'{folder}/{x}/{subx}/{item}')
      os.rmdir(f'{folder}/{x}/{subx}')
    if verbose == True:
      print(f'All items cleared from {folder}/{x}')


  #compile train, val sets
  np.random.seed(seed)
  minss = 10000
  for sp in selectspecies:
    sub = imgs[imgs.species==sp]
    nsub = len(sub)
    if nsub < minss:
      minss = nsub

  if verbose == True:
    print(f'\nAll species downsampled to {minss} \n')

  for sp in selectspecies:
    sub = imgs[imgs.species==sp]
    sub = sub.sample(n=minss, random_state=seed)
    subtrain = sub.iloc[int(len(sub)*split):].reset_index().drop(columns=['index'])
    subval = sub.iloc[:int(len(sub)*split)].reset_index().drop(columns=['index'])
    if sp == selectspecies[0]:
      imgs_train = subtrain
      imgs_val = subval
    else:
      imgs_train = imgs_train._append(subtrain).reset_index().drop(columns=['index'])
      imgs_val = imgs_val._append(subval).reset_index().drop(columns=['index'])

  if verbose == True:
    print(f'\nCreating training and validation sets...')
    print(f'Examples per species training set: {int(minss*(1-split))}')
    print(f'Examples per species validation set: {int(minss*split)}')

  traincount = {}
  valcount = {}
  for sp in selectspecies:
    examples_train = imgs_train[imgs_train.species==sp]
    examples_val = imgs_val[imgs_val.species==sp]

    if sp not in os.listdir(f'{folder}/train'):
      os.mkdir(f'{folder}/train/{sp}')
    if sp not in os.listdir(f'{folder}/val'):
      os.mkdir(f'{folder}/val/{sp}')

    traincount[sp] = 0
    for i in examples_train.index:
      file = examples_train.filename[i]
      src = f'{folder}/{subf}/{file}'
      dst = f'{folder}/train/{sp}/{file}'
      shutil.copy(src, dst)
      traincount[sp] += 1

    valcount[sp] = 0
    for i in examples_val.index:
      file = examples_val.filename[i]
      src = f'{folder}/{subf}/{file}'
      dst = f'{folder}/val/{sp}/{file}'
      shutil.copy(src, dst)
      valcount[sp] += 1

  if verbose == True:
    print('\nDone creating train and validation sets:')
    for sp in list(traincount.keys()):
      print(f'{sp} (Train): {traincount[sp]}')
      print(f'{sp} (Validation): {valcount[sp]}')

def syncshuffle(a, b):
    assert len(a) == len(b)
    p = np.random.permutation(len(a))
    return a[p], b[p]

def compile_data_from_csvpath(path, testenc, w=4, startcol=1, nmax=50, selectspecies=['Dde', 'Ggr', 'Gme', 'Lal', 'Ttr'], omit_encs=[], dd=None, max_ss=None, ss_exc_species=[], split=0.33, verbose=False, seed=42):
  data = pd.read_csv(path).iloc[:,startcol:]
  data = data[data.enc_id != 'Unk']
  data['enc_id'] = data['enc_id'].astype(int)
  data = data[data.species.isin(selectspecies)].reset_index().drop(columns=['index'])
  data = data[~data.enc_id.isin(omit_encs)].reset_index().drop(columns=['index'])
  newsp = ['Dde' if list(data.enc_id)[i]==474 else list(data.species)[i] for i in range(0, len(data))]
  data['species'] = newsp

  if 'dd' in data.columns and dd is not None:
    data = data[(data.dd >= dd[0]) & (data.dd <= dd[1])].reset_index().drop(columns=['index'])

  test = data[data.enc_id == testenc].reset_index().drop(columns=['index'])

  if 'dd' in test:
    test = test[test.dd != 'aug'].reset_index().drop(columns=['index'])

  data = data[data.enc_id != testenc].reset_index().drop(columns=['index'])

  data_ = pd.DataFrame()
  for enc in np.unique(data.enc_id):
    sub = data[data.enc_id==enc]
    if len(sub) > nmax:
      sub = sub.sample(n=nmax, random_state=seed)
    data_ = data_._append(sub).reset_index().drop(columns=['index'])

  data = data_.sort_index()
  nmin = 1000000
  for sp in np.unique(data.species):
    L = len(data[data.species == sp])
    if L < nmin:
      nmin = L

  data_ = pd.DataFrame()
  for sp in np.unique(data.species):
    sub = data[data.species == sp].reset_index().drop(columns=['index'])
    while len(sub) > nmin:
      maxenc = sub.groupby('enc_id').count().iloc[:,:1].sort_values(by='species').index[-1]
      dropind = sub[sub.enc_id == maxenc].sample(frac=1, random_state=seed).index[0]
      sub = sub.drop(dropind)

    data_ = data_._append(sub).reset_index().drop(columns=['index'])

  data = data_.sort_index()

  if max_ss is not None:
    if 'n' in data.columns:
      ind = np.where(data.columns=='0')[0][0]
    else:
      ind = np.where(data.columns=='2100')[0][0]
    newdata = pd.DataFrame()
    for enc in np.unique(data.enc_id):
      sub = data[data.enc_id == enc].reset_index().drop(columns=['index'])
      sp = list(sub.species)[0]
      if sp not in ss_exc_species:
        subsp = datap[datap.species == sp]
        medspectrum = np.median(sub.iloc[:,ind:], axis=0)
        medspectrum_sp = np.median(subsp.iloc[:,ind:], axis=0)
        b = medspectrum_sp
        spectra = [sub.iloc[n, ind:].values for n in range(0, len(sub))]
        ss = [100*np.mean((a-b)**2/(np.mean(b)**2)) for a in spectra]
        sub['ss'] = ss
        newdata = newdata._append(sub).reset_index().drop(columns=['index'])

    for sp in ss_exc_species:
      subsp = datap[datap.species == sp]
      newdata = newdata._append(subsp).reset_index().drop(columns=['index'])

    data = newdata[newdata.ss < max_ss].iloc[:,:-1].reset_index().drop(columns=['index'])

  rng = 100
  while rng > 20:
    train = data[data.enc_id != testenc].sample(frac=1).reset_index().drop(columns=['index'])
    ind = int(len(train)*split)
    val = train.iloc[:ind, :].reset_index().drop(columns=['index'])
    train = train.iloc[ind:, :].reset_index().drop(columns=['index'])
    rng = np.max(train.groupby('species').count().iloc[:,:1]) - np.min(train.groupby('species').count().iloc[:,:1])

  train, val = (train.sample(frac=1), val.sample(frac=1))
  if 'sel_id' not in train.columns:
    ind = np.where(train.columns=='rec_id')[0][0]
    train.insert(int(ind), 'sel_id', np.linspace(1,len(train),len(train)))

  if 'sel_id' not in val.columns:
    ind = np.where(val.columns=='rec_id')[0][0]
    val.insert(int(ind), 'sel_id', np.linspace(1,len(val),len(val)))

  if 'sel_id' not in test.columns:
    ind = np.where(test.columns=='rec_id')[0][0]
    test.insert(int(ind), 'sel_id', np.linspace(1,len(test),len(test)))

  if 'n' in train.columns:
    info_train = pd.DataFrame({'species': train['species'], 'enc_id': train['enc_id'], 'rec_id': train['rec_id'], 'sel_id': train['sel_id'], 'starttime':train['starttime'], 'n':train['n'], 'filename':[0]*len(train), 'site':['Other']*len(train)})
    info_val = pd.DataFrame({'species': val['species'], 'enc_id': val['enc_id'], 'rec_id': val['rec_id'], 'sel_id': val['sel_id'], 'starttime':val['starttime'], 'n':val['n'], 'filename':[0]*len(val), 'site':['Other']*len(val)})
    info_test = pd.DataFrame({'species': test['species'], 'enc_id': test['enc_id'], 'rec_id': test['rec_id'], 'sel_id': test['sel_id'], 'starttime':test['starttime'], 'n':test['n'], 'filename':[0]*len(test), 'site':['Other']*len(test)})
  else:
    info_train = pd.DataFrame({'species': train['species'], 'enc_id': train['enc_id'], 'rec_id': train['rec_id'], 'sel_id': train['sel_id'], 'starttime':train['starttime'], 'dd':train['dd'], 'site':['Other']*len(train)})
    info_val = pd.DataFrame({'species': val['species'], 'enc_id': val['enc_id'], 'rec_id': val['rec_id'], 'sel_id': val['sel_id'], 'starttime':val['starttime'], 'dd':val['dd'], 'site':['Other']*len(val)})
    info_test = pd.DataFrame({'species': test['species'], 'enc_id': test['enc_id'], 'rec_id': test['rec_id'], 'sel_id': test['sel_id'], 'starttime':test['starttime'], 'dd':test['dd'], 'site':['Other']*len(test)})

  encs = list(test['enc_id'])
  filenames = list(test['rec_id'])

  ytrain = np.array(train['species'])
  yval = np.array(val['species'])
  ytest = np.array(test['species'])

  if 'SNR' in train.columns:
    ind = np.where(data.columns=='X1')[0][0]+1
  elif 'n' in train.columns:
    ind = np.where(data.columns=='0')[0][0]+1
  elif 'X1' in train.columns:
    ind = np.where(data.columns=='X1')[0][0]+1
  else:
    ind = np.where(data.columns=='2100')[0][0]

  xtrain = np.array(train.iloc[:, ind:])
  xtrain = np.expand_dims(xtrain, axis=2)
  xtrain/(xtrain.sum(axis=1)[:,None]) #make sure each example is sum normalised
  xval = np.array(val.iloc[:, ind:])
  xval = np.expand_dims(xval, axis=2)
  xval/(xval.sum(axis=1)[:,None]) #make sure each example is sum normalised
  xtest = np.array(test.iloc[:, ind:])
  xtest = np.expand_dims(xtest, axis=2)
  xtest/(xtest.sum(axis=1)[:,None]) #make sure each example is sum normalised

  from sklearn.preprocessing import OneHotEncoder
  onehotencoder = OneHotEncoder()
  ytrain = onehotencoder.fit_transform(ytrain.reshape(-1,1)).toarray()
  yval = onehotencoder.transform(yval.reshape(-1,1)).toarray()

  if verbose == True:
    print(f'X Training: {xtrain.shape}')
    print(f'X Validation: {xval.shape}')
    print(f'Y Training: {ytrain.shape}')
    print(f'Y Validation: {yval.shape}')

  return xtrain, xval, xtest, ytrain, yval, ytest, filenames, encs, info_train, info_val, info_test

def initialise_model(img_shape=(48,64,1),
                     resize=10,
                     batch_size=2,
                     epochs=100,
                     lr=0.01,
                     partitions=1,
                     classes=7,
                     nfiltersconv=16,
                     kernelconv=5,
                     padding='same',
                     maxpool=4,
                     leaky=0.1,
                     densesize=8,
                     l2=0.0001,
                     dropout=0.5,
                     patience=25):

  model_2s = Sequential()
  model_2s.add(Conv1D(nfiltersconv, kernel_size=kernelconv, activation='linear', input_shape=img_shape, padding=padding))
  model_2s.add(MaxPooling1D(maxpool,padding=padding))
  model_2s.add(LeakyReLU(alpha=leaky))
  model_2s.add(Flatten())
  model_2s.add(Dense(densesize, activation='linear', kernel_regularizer=regularizers.l2(l2)))
  model_2s.add(Dropout(dropout))
  model_2s.add(Dense(classes, activation='softmax'))

  model_4s = Sequential()
  model_4s.add(Conv1D(nfiltersconv, kernel_size=kernelconv, activation='linear', input_shape=img_shape, padding=padding))
  model_4s.add(MaxPooling1D(maxpool,padding=padding))
  model_4s.add(Conv1D(nfiltersconv, kernel_size=kernelconv+2, activation='linear', input_shape=img_shape, padding=padding))
  model_4s.add(MaxPooling1D(maxpool,padding=padding))
  model_4s.add(LeakyReLU(alpha=leaky))
  model_4s.add(Flatten())
  model_4s.add(Dense(densesize, activation='linear', kernel_regularizer=regularizers.l2(l2)))
  model_4s.add(Dropout(dropout))
  model_4s.add(Dense(classes, activation='softmax'))

  model_8s = Sequential()
  model_8s.add(Conv1D(nfiltersconv, kernel_size=kernelconv, activation='linear', input_shape=img_shape, padding=padding))
  model_8s.add(MaxPooling1D(maxpool,padding=padding))
  model_8s.add(LeakyReLU(alpha=leaky))
  model_8s.add(Flatten())
  model_8s.add(Dense(densesize, activation='linear', kernel_regularizer=regularizers.l2(l2)))
  model_8s.add(Dropout(dropout))
  model_8s.add(Dense(classes, activation='softmax'))

  import keras.backend as K

  model_2s.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])
  model_4s.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])
  model_8s.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])
  es = EarlyStopping('val_loss', mode = 'min', verbose=1, patience = patience)

  def scheduler(epoch, lr):
    if epoch < 30:
      return 0.0005
    else:
      return 0.00025

  lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)

  return model_2s, model_4s, model_8s, es, lr_schedule

def predict_test_singlemodel(model, xtest, ytest, testenc, selectspecies=['Dde', 'Ggr', 'Gme', 'Lac', 'Lal', 'Oor', 'Ttr'], w=2, verbose=True):

  preds = model.predict(xtest, verbose=0)

  d = {'id':[], 'x':[], 'enc_id':[], 'filename':[], 'label':[], 'pred':[], 'conf':[]}
  for sp in selectspecies:
    d[sp] = []
  cum = {}
  for sp in selectspecies:
    cum[sp] = 0

  id = 1
  for i in range(0, len(preds)):
    d['id'].append(id)
    d['x'].append(xtest[i])
    d['enc_id'].append(encs[i])
    d['filename'].append(filenames[i])
    d['label'].append(ytest[i])
    d['pred'].append(np.unique(selectspecies)[np.argmax(preds[i])])
    d['conf'].append(np.max(preds[i]))
    for j in range(0, len(selectspecies)):
      sp = selectspecies[j]
      cum[selectspecies[j]] += preds[i][j]
      d[sp].append(preds[i][j])

  cum_total = np.sum(list(cum.values()))
  for sp in selectspecies:
    cum[sp] = cum[sp]/cum_total

  df_preds = pd.DataFrame(d)

  predicted_species = selectspecies[np.argmax(list(cum.values()))]

  conf_matrix = pd.crosstab(df_preds['label'], df_preds['pred'], rownames=['True'], colnames=['Predicted'])

  from colorama import Fore
  if verbose == True:
    if ytest[0] == predicted_species:
      result = 'correct'
      print(f'Encounter {testenc} ({w} sec res.) - - - ' + Fore.GREEN + f'Prediction {result} (True: {ytest[0]}, Predicted: {predicted_species})')
    else:
      result = 'incorrect'
      print(f'Encounter {testenc} ({w} sec res.) - - - ' + Fore.RED + f' Prediction {result} (True: {ytest[0]}, Predicted: {predicted_species})')
  print(Fore.BLACK)

  return df_preds, cum, predicted_species, ytest[0], conf_matrix

def savetolog(folder, filename, locs, info_train, info_val,
              testsp=None, testenc=None, w=None, epochs=None, batch_size=None, resize=None, patience=None, model=None, nfiltersconv=None, kernelconv=None,
              padding=None, maxpool=None, leaky=None, densesize=None, dropout=None, trainencs=None, valencs=None, THR=None, nmax=None, minval=None,
              xtrain=None, xtest=None, ytrain=None, ytest=None, encs_t=None, encs_v=None, conf_matrix=None, cum=None, acc=None, loss=None, valacc=None, valloss=None,
              initial=False, verbose=False):
  d = {}
  allspecies = ['Dde', 'Ggr', 'Gme', 'Lac', 'Lal', 'Oor', 'Ttr']

  for species in sorted(allspecies):
    d[species] = [species in selectspecies]
  d['testspecies'] = [testsp]
  d['testenc'] = [testenc]
  d['time_res'] = [w]
  d['epochs_max'] = [epochs]
  d['epochs_used'] = [len(valloss)]
  d['batch_size'] = batch_size
  d['compress_factor'] = resize
  d['lrs'] = [[0.001, 0.0005]]
  d['lr_epochchange'] = [30]
  d['patience'] = [patience]
  d['n_layers'] = [len(model.layers)]
  d['layers'] = [model.layers]
  d['nfiltersConv'] = [nfiltersconv]
  d['kernelSize'] = [kernelconv]
  d['padding'] = [padding]
  d['maxPool'] = [maxpool]
  d['leakyReLu'] = [leaky]
  d['denseSize'] = [densesize]
  d['dropout'] = [dropout]

  encs_t = []
  if trainencs is not None:
    for sp in list(trainencs.keys()):
      for item in trainencs[sp]:
        encs_t.append(item)

  encs_v = []
  if valencs is not None:
    for sp in list(valencs.keys()):
      for item in valencs[sp]:
        encs_v.append(item)

  d['min_enc'] = [THR]
  d['max_trainenc'] = [nmax]
  d['min_valset'] = [minval]
  d['ntrain'] = [len(xtrain)]
  d['nval'] = [len(xval)]
  d['ntest'] = [len(xtest)]
  d['n_encstrain'] = [len(encs_t)]
  d['n_encsval'] = [len(encs_v)]
  d['encstrain'] = [encs_t]
  d['encsval'] = [encs_v]

  info_train_sub = info_train[info_train.species == testsp]
  info_val_sub = info_val[info_val.species == testsp]
  for site in list(locs.keys()):
    tot_train = len(info_train_sub)
    tot_val = len(info_val_sub)
    count_train = np.sum(info_train_sub.site == site)
    count_val = np.sum(info_val_sub.site == site)
    d[f'trainsite_{site}'] = [count_train/tot_train]
    d[f'valsite_{site}'] = [count_val/tot_val]

  tot = 0
  for species in sorted(allspecies):
    if species in conf_matrix.columns:
      d[f'pred{species}'] = [conf_matrix[species][0]]
      tot += conf_matrix[species][0]
    else:
      d[f'pred{species}'] = [0]
      tot += 0

  d['predTOTAL'] = [tot]

  for species in sorted(allspecies):
    if species in selectspecies:
      d[f'aggpred{species}'] = [round(cum[species], 3)]
    else:
      d[f'aggpred{species}'] = [0]

  acc10_end = np.mean(acc[-10:])
  acc10_start = np.mean(acc[:10])
  valacc10_end = np.mean(valacc[-10:])
  valacc10_start = np.mean(valacc[:10])
  loss10_end = np.mean(loss[-10:])
  loss10_start = np.mean(loss[:10])
  valloss10_end = np.mean(valloss[-10:])
  valloss10_start = np.mean(valloss[:10])

  d['acctrain_10'] = [round(acc10_end,3)]
  d['acctrainstd_10'] = [round(np.std(acc[-10:]),3)]
  d['accval_10'] = [round(valacc10_end,3)]
  d['accvalstd_10'] = [round(np.std(valacc[-10:]),3)]
  d['losstrain_10'] = [round(loss10_end,3)]
  d['losstrainstd_10'] = [round(np.std(loss[-10:]),3)]
  d['lossval_10'] = [round(valloss10_end,3)]
  d['lossvalstd_10'] = [round(np.std(valloss[-10:]),3)]
  d['acctrain_trend'] = [round((acc10_end-acc10_start)/(len(acc)-10),3)]
  d['accval_trend'] = [round((valacc10_end-valacc10_start)/(len(valacc)-10),3)]
  d['losstrain_trend'] = [round((loss10_end-loss10_start)/(len(loss)-10),3)]
  d['valloss_trend'] = [round((valloss10_end-valloss10_start)/(len(valloss)-10),3)]

  if initial == True:
    mode = 'w'
    header = True
  else:
    mode = 'a'
    header = False

  pd.DataFrame(d).to_csv(f'{folder}/{filename}', index=True, header=header, mode=mode)
  if verbose == True:
    print(f'Saved results to {folder}/{filename}')